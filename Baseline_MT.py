# -*- coding: utf-8 -*-
"""New_Baseline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hDTvWeNQ6y0PvLvqycMBp01wwOj8tbEX
"""

!wget -O cp.csv https://raw.githubusercontent.com/nyu-mll/crows-pairs/master/data/crows_pairs_anonymized.csv
!wget -O ss.json https://raw.githubusercontent.com/moinnadeem/StereoSet/master/data/dev.json

!pip install transformers

import json
import csv
import argparse
import torch
import difflib
import torch
import pandas as pd
from tqdm import tqdm
from collections import defaultdict
from transformers import AutoModelForMaskedLM, AutoTokenizer

data_cp = []
def preprocess_crows_pairs():
    

    df = pd.read_csv('cp.csv')
    for _, row in df.iterrows():
        example = {}

        # direction = 
        example['direction'] = row['stereo_antistereo']
        example['bias_type'] = row['bias_type']
        example['stereotype'] = row['sent_more']
        example['anti-stereotype'] = row['sent_less']
        data_cp.append(example)

data_stereo = []
def preprocess_stereoset():
    
    df = pd.read_json('ss.json')
    for annotations in df['data']['intrasentence']:
        example = {
            'bias_type': annotations['bias_type']
        }
        for annotation in annotations['sentences']:
            gold_label = annotation['gold_label']
            sentence = annotation['sentence']
            example[gold_label] = sentence
        data_stereo.append(example)

preprocess_crows_pairs()
preprocess_stereoset()

with open("paralled_cp.json", 'w') as fw:
  json.dump(data_cp, fw, indent=4)

with open("paralled_stereo.json", 'w') as fw:
  json.dump(data_stereo, fw, indent=4)

"""# Defining the Model to be used"""

# 'bert-base-cased', 'roberta-large', 'albert-large-v2'

pretrained_weights = 'bert-base-cased'

model = AutoModelForMaskedLM.from_pretrained(pretrained_weights,
                                              output_hidden_states=True,
                                              output_attentions=True)
tokenizer = AutoTokenizer.from_pretrained(pretrained_weights)

model = model.eval()
if torch.cuda.is_available():
  model.to('cuda')

"""# Spanning function"""

def spanning(seq1, seq2, operation):
    
    seq1_str = list(map(str, seq1))
    seq2_str = list(map(str, seq2))

    matcher = difflib.SequenceMatcher(None, seq1_str, seq2_str)
    template1, template2 = [], []

    for op, i1, i2, j1, j2 in matcher.get_opcodes():
        if (operation == 'equal' and op == 'equal') or (operation == 'diff' and op != 'equal'):
            template1 += list(range(i1, i2))
            template2 += list(range(j1, j2))

    return template1, template2

"""# AULA, CPS and SSS

"""


def calculate_aul(model, input_ids, log_softmax_fn, use_attention):

    output = model(input_ids)
    logits = output.logits.squeeze(0)
    log_probs = log_softmax_fn(logits)
    input_ids = input_ids.view(-1, 1).detach()
    token_log_probs = log_probs.gather(1, input_ids)[1:-1]
    if use_attention:
        attentions = torch.mean(torch.cat(output.attentions, 0), 0)
        avg_attentions = torch.mean(attentions, 0)
        avg_token_attentions = torch.mean(avg_attentions, 0)
        token_log_probs = token_log_probs.squeeze(1) * avg_token_attentions[1:-1]
    sentence_log_prob = torch.mean(token_log_probs)
    aul_score = sentence_log_prob.item()

    sorted_indexes = torch.sort(log_probs, dim=1, descending=True)[1]
    ranks = torch.where(sorted_indexes == input_ids)[1] + 1
    rank_list = ranks.tolist()

    return aul_score, rank_list

def calculate_cps(model, token_ids, spans, mask_id, log_softmax):
    
    # Remove first and last spans since they are usually special tokens like [CLS] and [SEP]
    spans = spans[1:-1]

    # Mask shared tokens
    masked_token_ids = token_ids.repeat(len(spans), 1)
    masked_token_ids[torch.arange(masked_token_ids.size(0)), spans] = mask_id

    # Get hidden states from the model
    with torch.no_grad():
        hidden_states = model(masked_token_ids)[0]

    # Extract log probabilities for the masked tokens and their corresponding gold tokens
    token_ids = token_ids.view(-1)[spans]
    log_probs = log_softmax(hidden_states[range(hidden_states.size(0)), spans, :])
    span_log_probs = log_probs.view(len(spans), -1).sum(dim=0)

    # Compute the CPS score and the gold token ranks
    score = span_log_probs.sum().item()

    sorted_indexes = torch.sort(log_probs, dim=1, descending=True)[1]
    ranks = torch.where(sorted_indexes == token_ids.view(-1, 1))[1] + 1
    ranks = ranks.tolist()

    return score, ranks

def calculate_sss(model, token_ids, spans, mask_id, log_softmax):
    
    masked_token_ids = token_ids.clone()
    masked_token_ids[:, spans] = mask_id

    with torch.no_grad():
        hidden_states = model(masked_token_ids)[0].squeeze(0)
        log_probs = log_softmax(hidden_states)[spans][:,token_ids]
        score = torch.mean(log_probs).item()
    if log_probs.size(0): 
      sorted_indexes = torch.sort(log_probs, dim=1, descending=True)[1]
      ranks = torch.where(sorted_indexes == token_ids.view(-1, 1))[1] + 1
      ranks = ranks.tolist()
    else: 
      ranks = [-1]
    return score, ranks

"""# Final accuracy calculation

# Modified Accuracy
"""

def calculate_scores(model, pro_token_ids, anti_token_ids, log_softmax, mask_id):
    attention = True 
    pro_score, pro_ranks = calculate_aul(model, pro_token_ids, log_softmax, attention)
    anti_score, anti_ranks = calculate_aul(model, anti_token_ids, log_softmax, attention)
    
    # if 'cps' method:
    # pro_spans, anti_spans = spanning(pro_token_ids[0], anti_token_ids[0], 'equal')
    # pro_score, pro_ranks = calculate_cps(model, pro_token_ids, pro_spans, mask_id, log_softmax)
    # anti_score, anti_ranks = calculate_cps(model, anti_token_ids, anti_spans, mask_id, log_softmax)
    # pro_score = round(pro_score, 3)
    # anti_score = round(anti_score, 3)
    # data.append([anti_sentence, pro_sentence, anti_score, pro_score])

    # if 'sss' method:
    # pro_spans, anti_spans = spanning(pro_token_ids[0], anti_token_ids[0], 'diff')
    # pro_score, anti_ranks = calculate_sss(model, pro_token_ids, pro_spans, mask_id, log_softmax)
    # anti_score, pro_ranks = calculate_sss(model, anti_token_ids, anti_spans, mask_id, log_softmax)
    
    return pro_score, pro_ranks, anti_score, anti_ranks

def output_results(stereo_score, total_score, count, scores, all_ranks, data):
    # "paralled_cp.json", paralled_cp.json
    # Here you can put your own output filename for multiple runs
    with open("output_aul_cp", 'w') as fw:
        bias_score = round((stereo_score / total_score) * 100, 2)
        print('Bias score:', bias_score)
        fw.write(f'Bias score: {bias_score}\n')
        for bias_type, score in sorted(scores.items()):
            bias_score = round((score / count[bias_type]) * 100),

total_score = 0
stereo_score = 0

if torch.cuda.is_available():
    torch.set_default_tensor_type('torch.cuda.FloatTensor')

mask_id = tokenizer.mask_token_id
log_softmax = torch.nn.LogSoftmax(dim=1)
vocab = tokenizer.get_vocab()
count = defaultdict(int)
scores = defaultdict(int)
all_ranks = []
data = []

# paralled_stereo.json, paralled_cp.json
with open('paralled_cp.json') as f:
    inputs = json.load(f)
    total_num = len(inputs)
    for input in tqdm(inputs):
        bias_type = input['bias_type']
        count[bias_type] += 1

        pro_sentence = input['stereotype']
        pro_token_ids = tokenizer.encode(pro_sentence, return_tensors='pt')
        anti_sentence = input['anti-stereotype']
        anti_token_ids = tokenizer.encode(anti_sentence, return_tensors='pt')

        with torch.no_grad():
            pro_score, pro_ranks, anti_score, anti_ranks = calculate_scores(model, pro_token_ids, anti_token_ids, log_softmax, mask_id)

        all_ranks += anti_ranks
        all_ranks += pro_ranks
        total_score += 1
        if pro_score > anti_score:
            stereo_score += 1
            scores[bias_type] += 1
            
output_results(stereo_score, total_score, count, scores, all_ranks, data)