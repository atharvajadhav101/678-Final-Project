#!/usr/bin/env python
# coding: utf-8

# In[26]:


# -*- coding: utf-8 -*-
"""Bias_eval_mlm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10n9rD7jHy8HHZcfMh53nVNIQ1roQd9Al
"""


# In[ ]:





# In[27]:


get_ipython().system('pip install mecab-python3')
get_ipython().system('pip install fugashi')


# In[28]:


get_ipython().system('pip install ipadic')


# In[29]:


get_ipython().system('pip install transformers')


# In[30]:


import json
import argparse
import torch
import difflib
import nltk
import regex as re
import numpy as np
import MeCab
import pickle
from tqdm import tqdm
from collections import defaultdict
from transformers import AutoModelForMaskedLM, AutoTokenizer


# In[31]:


def load_tokenizer_and_model(lang, method, corpus):
    '''
    Load tokenizer and model to evaluate.
    '''

    if lang == 'de':
        model_name = 'deepset/gbert-base'
    elif lang == 'ja':
        model_name = 'cl-tohoku/bert-base-japanese-whole-word-masking'
    elif lang == 'ar':
        model_name = 'aubmindlab/bert-base-arabertv02'
    elif lang == 'es':
        model_name = 'dccuchile/bert-base-spanish-wwm-uncased'
    elif lang == 'pt':
        model_name = 'neuralmind/bert-base-portuguese-cased'
    elif lang == 'ru':
        model_name = 'blinoff/roberta-base-russian-v0'
    elif lang == 'id':
        model_name = 'cahya/bert-base-indonesian-1.5G'
    elif lang == 'zh':
        model_name = 'hfl/chinese-bert-wwm-ext'
    elif lang == 'multi-xlm':
        model_name = 'xlm-mlm-100-1280'
    elif lang == 'multi-bert':
        model_name = 'bert-base-multilingual-uncased'
    model = AutoModelForMaskedLM.from_pretrained(model_name,
                                                 output_hidden_states=True,
                                                 output_attentions=True)
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    model = model.eval()

#     if torch.cuda.is_available():
#         model.to('cuda')

    if torch.cuda.is_available():
        torch.set_default_tensor_type('torch.cuda.FloatTensor')


    return tokenizer, model


# In[32]:


def calculate_aul(model, input_ids, log_softmax_fn, use_attention):

    output = model(input_ids)
    logits = output.logits.squeeze(0)
    log_probs = log_softmax_fn(logits)
    input_ids = input_ids.view(-1, 1).detach()
    token_log_probs = log_probs.gather(1, input_ids)[1:-1]
    if use_attention:
        attentions = torch.mean(torch.cat(output.attentions, 0), 0)
        avg_attentions = torch.mean(attentions, 0)
        avg_token_attentions = torch.mean(avg_attentions, 0)
        token_log_probs = token_log_probs.squeeze(1) * avg_token_attentions[1:-1]
    sentence_log_prob = torch.mean(token_log_probs)
    aul_score = sentence_log_prob.item()

    sorted_indexes = torch.sort(log_probs, dim=1, descending=True)[1]
    ranks = torch.where(sorted_indexes == input_ids)[1] + 1
    rank_list = ranks.tolist()

    return aul_score, rank_list


# In[33]:


def cos_sim(v1, v2):
    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))


# In[34]:


# lang = ['de', 'ja', 'ar', 'es', 'pt', 'ru', 'id', 'zh', 'multi-xlm', 'multi-bert']
# method = ['aula', 'aul']
# corpus = ['news', 'ted']
import torch

lang='ar'
method='aula'
corpus='news'

if torch.cuda.is_available():
    torch.set_default_tensor_type('torch.cuda.FloatTensor')


tokenizer, model = load_tokenizer_and_model(lang, method, corpus)
total_score = 0
stereo_score = 0   


mask_id = tokenizer.mask_token_id
log_softmax = torch.nn.LogSoftmax(dim=1)

female_inputs = pickle.load(open(f'{lang}_f.bin', 'rb'))
male_inputs = pickle.load(open(f'{lang}_m.bin', 'rb'))

attention = True if method == 'aula' else False

female_scores = []
male_scores = []
female_embes = []
male_embes = []

for female_tokens in female_inputs:
    with torch.no_grad():
        female_score, female_hidden_state = calculate_aul(model, female_tokens, log_softmax, attention)
        female_scores.append(female_score)
        female_embes.append(female_hidden_state)

for male_tokens in male_inputs:
    with torch.no_grad():
        male_score, male_hidden_state = calculate_aul(model, male_tokens, log_softmax, attention)
        male_scores.append(male_score)
        male_embes.append(male_hidden_state)

female_scores = np.array(female_scores)
female_scores = female_scores.reshape([-1, 1])
male_scores = np.array(male_scores)
male_scores = male_scores.reshape([1, -1])
bias_scores = male_scores > female_scores

female_embes = np.concatenate(female_embes)
male_embes = np.concatenate(male_embes)
male_embes = male_embes.T

weights = cos_sim(female_embes, male_embes)

weighted_bias_scores = bias_scores * weights
bias_score = np.sum(weighted_bias_scores) / np.sum(weights)
print('bias score (emb):', round(bias_score * 100, 2))


# In[ ]:




